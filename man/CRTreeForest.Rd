% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CRTreeForest.R
\name{CRTreeForest}
\alias{CRTreeForest}
\title{Complete-Random Tree Forest implementation in R}
\usage{
CRTreeForest(training_data, validation_data, training_labels, validation_labels,
  folds, nthread = 1, lr = 1, training_start = NULL,
  validation_start = NULL, n_forest = 5, n_trees = 1000,
  random_forest = 0, seed = 0, objective = "reg:linear",
  eval_metric = Laurae::df_rmse, return_list = TRUE, multi_class = 2,
  verbose = " ", garbage = FALSE)
}
\arguments{
\item{training_data}{Type: data.table. The training data.}

\item{validation_data}{Type: data.table. The validation data with labels to check for metric performance. Set to \code{NULL} if you want to use out of fold validation data instead of a custom validation data set.}

\item{training_labels}{Type: numeric vector. The training labels.}

\item{validation_labels}{Type: numeric vector. The validation labels.}

\item{folds}{Type: list. The folds as list for cross-validation.}

\item{nthread}{Type: numeric. The number of threads using for multithreading. 1 means singlethread (uses only one core). Higher may mean faster training if the memory overhead is not too large. Defaults to \code{1}.}

\item{lr}{Type: numeric. The shrinkage affected to each tree to avoid overfitting. Defaults to \code{1}, which means no adjustment.}

\item{training_start}{Type: numeric vector. The initial training prediction labels. Set to \code{NULL} if you do not know what you are doing. Defaults to \code{NULL}.}

\item{validation_start}{Type: numeric vector. The initial validation prediction labels. Set to \code{NULL} if you do not know what you are doing. Defaults to \code{NULL}.}

\item{n_forest}{Type: numeric. The number of forest models to create for the Complete-Random Tree Forest. Defaults to \code{5}.}

\item{n_trees}{Type: numeric. The number of trees per forest model to create for the Complete-Random Tree Forest. Defaults to \code{1000}.}

\item{random_forest}{Type: numeric. The number of Random Forest in the forest. Defaults to \code{0}.}

\item{seed}{Type: numeric. Random seed for reproducibility. Defaults to \code{0}.}

\item{objective}{Type: character or function. The function which leads \code{boosting} loss. See \code{xgboost::xgb.train}. Defaults to \code{"reg:linear"}.}

\item{eval_metric}{Type: function. The function which evaluates \code{boosting} loss. Must take two arguments in the following order: \code{preds, labels} (they may be named in another way) and returns a metric. Defaults to \code{Laurae::df_rmse}.}

\item{return_list}{Type: logical. Whether lists should be returned instead of concatenated frames for predictions. Defaults to \code{TRUE}.}

\item{multi_class}{Type: numeric. Defines the number of classes internally for whether you are doing multi class classification or not to use specific routines for multiclass problems when using \code{return_list == FALSE}. Defaults to \code{2}, which is for regression and binary classification.}

\item{verbose}{Type: character. Whether to print for training evaluation. Use \code{""} for no printing (double quotes without space between quotes). Defaults to \code{" "} (double quotes with space between quotes.}

\item{garbage}{Type: logical. Whether to perform garbage collect regularly. Defaults to \code{FALSE}.}
}
\value{
A data.table based on \code{target}.
}
\description{
This function attempts to replicate Complete-Random Tree Forests using xgboost. It performs Random Forest \code{n_forest} times using \code{n_trees} trees. You can specify your learning objective using \code{objective} and the metric to check for using \code{eval_metric}. You can plug custom objectives instead of the objectives provided by \code{xgboost}. As with any uncalibrated machine learning methods, this method suffers uncalibrated outputs. Therefore, the usage of scale-dependent metrics is discouraged (please use scale-invariant metrics, such as Accuracy, AUC, R-squared, Spearman correlation...).
}
\details{
For implementation details of Cascade Forest / Complete-Random Tree Forest / Multi-Grained Scanning / Deep Forest, check this: \url{https://github.com/Microsoft/LightGBM/issues/331#issuecomment-283942390} by Laurae.

Actually, this function creates a layer of a Cascade Forest. That layer is comprised of two possible elements: Complete-Random Tree Forests (using PFO mode: Probability Averaging + Full Height + Original training samples) and Random Forests. You may choose between them.

Complete-Random Tree Forests in PFO mode are the best random learners inside the Complete-Random Tree Forest families (at least 50% higher winrate against other families, including Random Forest). The major issue is their randomness which lowers their performance until they are fully extended for maximum performance: it takes a long time to train them properly until the features are so obvious they learn nearly instantly in one run of training. Therefore, they are extremely prone to underfitting, and a \code{CascadeForest} should be used to improve their performance combined with one or multiple Random Forest.

Laurae recommends using xgboost or LightGBM on top of gcForest or Cascade Forest. See the rationale here: \url{https://github.com/Microsoft/LightGBM/issues/331#issuecomment-284689795}.
}
\examples{
\dontrun{
# Load libraries
library(data.table)
library(Matrix)
library(xgboost)

# Create data
data(agaricus.train, package = "lightgbm")
data(agaricus.test, package = "lightgbm")
agaricus_data_train <- data.table(as.matrix(agaricus.train$data))
agaricus_data_test <- data.table(as.matrix(agaricus.test$data))
agaricus_label_train <- agaricus.train$label
agaricus_label_test <- agaricus.test$label
folds <- Laurae::kfold(agaricus_label_train, 5)

# Train a model (binary classification)
model <- CRTreeForest(training_data = agaricus_data_train, # Training data
                      validation_data = agaricus_data_test, # Validation data
                      training_labels = agaricus_label_train, # Training labels
                      validation_labels = agaricus_label_test, # Validation labels
                      folds = folds, # Folds for cross-validation
                      nthread = 1, # Change this to use more threads
                      lr = 1, # Do not touch this unless you are expert
                      training_start = NULL, # Do not touch this unless you are expert
                      validation_start = NULL, # Do not touch this unless you are expert
                      n_forest = 5, # Number of forest models
                      n_trees = 10, # Number of trees per forest
                      random_forest = 2, # We want only 2 random forest
                      seed = 0,
                      objective = "binary:logistic",
                      eval_metric = Laurae::df_logloss,
                      return_list = TRUE, # Set this to FALSE for a data.table output
                      multi_class = 2, # Modify this for multiclass problems
                      verbose = " ")

# Attempt to perform fake multiclass problem
agaricus_label_train[1:100] <- 2

# Train a model (multiclass classification)
model <- CRTreeForest(training_data = agaricus_data_train, # Training data
                      validation_data = agaricus_data_test, # Validation data
                      training_labels = agaricus_label_train, # Training labels
                      validation_labels = agaricus_label_test, # Validation labels
                      folds = folds, # Folds for cross-validation
                      nthread = 1, # Change this to use more threads
                      lr = 1, # Do not touch this unless you are expert
                      training_start = NULL, # Do not touch this unless you are expert
                      validation_start = NULL, # Do not touch this unless you are expert
                      n_forest = 5, # Number of forest models
                      n_trees = 10, # Number of trees per forest
                      random_forest = 2, # We want only 2 random forest
                      seed = 0,
                      objective = "multi:softprob",
                      eval_metric = Laurae::df_logloss,
                      return_list = TRUE, # Set this to FALSE for a data.table output
                      multi_class = 3, # Modify this for multiclass problems
                      verbose = " ")
}

}

