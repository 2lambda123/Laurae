% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LauraeML_gblinear.R
\name{LauraeML_gblinear}
\alias{LauraeML_gblinear}
\title{Laurae's Machine Learning (xgboost gblinear helper function)}
\usage{
LauraeML_gblinear(x, y, mobile, parallelized, maximize, logging, data, label,
  folds)
}
\arguments{
\item{x}{Type: vector (numeric). The hyperparameters to use.}

\item{y}{Type: vector (numeric). The features to use, as binary format (0 for not using, 1 for using).}

\item{mobile}{Type: environment. The environment passed from \code{LauraeML}.}

\item{parallelized}{Type: parallel socket cluster (makeCluster or similar). The \code{parallelized} parameter passed from \code{LauraeML} (whether to parallelize training per folds or not).}

\item{maximize}{Type: boolean. The \code{maximize} parameter passed from \code{LauraeML} (whether to maximize or not the metric).}

\item{logging}{Type: character. The \code{logging} parameter passed from \code{LauraeML} (where to store log file).}

\item{data}{Type: data.table (mandatory). The data features. Comes from \code{LauraeML}.}

\item{label}{Type: vector (numeric). The labels. Comes from \code{LauraeML}.}

\item{folds}{Type: list of numerics. The folds as list. Comes from \code{LauraeML}.}
}
\value{
The score of the cross-validated xgboost gblinear model, for the provided hyperparameters and features to use.
}
\description{
This function is a demonstration function for using xgboost gblinear in \code{LauraeML} without premade folds. It has \code{alpha}, \code{lambda}, and \code{lambda_bias} as tunable hyperparameters. It also accepts feature selection, and performs full logging (every part is commented in the source) with writing to an external file in order to follow the hyperparameters and feature count.
}
\examples{
\dontrun{
# To add
}

}

