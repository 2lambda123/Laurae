% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xgb.ncv.R
\name{xgb.ncv}
\alias{xgb.ncv}
\title{xgboost repeated cross-validation (Repeated k-fold)}
\usage{
xgb.ncv(data, label, extra_data = NA, out_of_fold = TRUE, nfolds = 5,
  ntimes = 3, nthread = 2, seed = 11111, verbose = 1,
  print_every_n = 1, sinkfile = "debug.txt", booster = "gbtree",
  eta = 0.3, max_depth = 6, min_child_weight = 1, gamma = 0,
  subsample = 1, colsample_bytree = 1, num_parallel_tree = 1,
  maximum_rounds = 1e+05, objective = "binary:logistic",
  eval_metric = "logloss", maximize = FALSE, early_stopping_rounds = 50)
}
\arguments{
\item{data}{The data as a matrix or sparse matrix.}

\item{label}{The label associated with the data.}

\item{extra_data}{The data you want to predict on using the fold models.}

\item{out_of_fold}{Should we predict out of fold? (this includes both \code{data} and \code{extra_data}). Defaults to \code{TRUE}.}

\item{nfolds}{How many folds should we use for the validation? The greater the better (increases linearly*ntimes the computation time. Defaults to \code{5}.}

\item{ntimes}{How many folds should we use? The greater the more stable results (increases linearly*nfolds the computation time.) Defaults to \code{3}.}

\item{nthread}{How many threads to run for xgboost? Defaults to \code{2}.}

\item{seed}{Which seed should we use globally for all commands dependent on a random seed? Defaults to \code{11111}.}

\item{verbose}{Should we print verbose data in xgboost? xgboost messages will be sinked in any case. Defaults to \code{1}.}

\item{print_every_n}{Every how many iterations should we print verbose data? xgboost messages will be sinked in any case.Defaults to \code{1}.}

\item{sinkfile}{What file name to give to the sink? This is where printed messages of xgboost will be stored.  Defaults to \code{"debug.txt"}.}

\item{booster}{What xgboost booster to use? Defaults to \code{"gbtree"} and must not be changed (does NOT work otherwise).}

\item{eta}{The shrinkage in xgboost. The lower the better, but increases exponentially the computation time as it gets lower. Defaults to \code{0.3}.}

\item{max_depth}{The maximum depth of each tree in xgboost. Defaults to \code{6}.}

\item{min_child_weight}{The minimum hessian weight needed in a child node. Defaults to \code{1}.}

\item{gamma}{The minimum loss reduction needed in a child node. Defaults to \code{0}.}

\item{subsample}{The sampling ratio of observations during each iteration. Use \code{0.632} to simulate Random Forests. Defaults to \code{1}.}

\item{colsample_bytree}{The sampling ratio of features during each iteration. Defaults to \code{1}.}

\item{num_parallel_tree}{How many trees to grow per iteration? A number higher than \code{1} simulates boosted Random Forests. Defaults to \code{1}.}

\item{maximum_rounds}{How many rounds until giving up boosting if not stopped early? Defaults to \code{100000}.}

\item{objective}{The objective function. Defaults to \code{"binary:logistic"}.}

\item{eval_metric}{The evaluation metric. Defaults to \code{"logloss"}.}

\item{maximize}{Should we maximize the evaluation metric? Defaults to \code{FALSE}.}

\item{early_stopping_rounds}{How many rounds the evaluation metric does not follow the maximization rule to force stopping a boosting iteration of xgboost on a fold? Defaults to \code{50}.}
}
\value{
A list with two to four elements: \code{"scores"} for the scored folds (data.frame), \code{"folds"} for the folds IDs (list), \code{"preds"} for out of fold predictions (data.frame), and \code{"extra"} for extra data predictions per fold (data.frame).
}
\description{
This function allows you to run a repeated cross-validation using xgboost, to get out of fold predictions, and to get predictions from each fold on external data.
It currently does not work for non 1-column prediction (only works for binary classification and regression).
Verbosity is automatic and cannot be removed. In case you need this function without verbosity, please compile the package after removing verbose messages.
In addition, a sink is forced. Make sure to run \code{sink()} if you interrupt (or if xgboost interrupts) prematurely the execution of the function. Otherwise, you end up with no more messages printed to your R console.
}
\examples{
Pick your xgb.cv function, replace data by the initial matrix, insert the label,
check ntimes to the value you want, and change the sinkfile.
Unlist params if needed, and add the seed as a parameter.

}

