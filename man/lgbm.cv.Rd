% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lgbm.cv.R
\name{lgbm.cv}
\alias{lgbm.cv}
\title{LightGBM Cross-Validated Model Training}
\usage{
lgbm.cv(y_train, x_train, folds, application = "regression",
  validation = TRUE, num_iterations = 10, early_stopping_rounds = NA,
  learning_rate = 0.1, num_leaves = 127, tree_learner = "serial",
  num_threads = 2, min_data_in_leaf = 100, min_sum_hessian_in_leaf = 10,
  feature_fraction = 1, feature_fraction_seed = 2, bagging_fraction = 1,
  bagging_freq = 0, bagging_seed = 3, max_bin = 255,
  data_random_seed = 1, data_has_label = TRUE,
  output_model = "LightGBM_model.txt", input_model = NA,
  output_result = "LightGBM_predict_result.txt", is_sigmoid = TRUE,
  init_score = "", is_pre_partition = FALSE, is_sparse = TRUE,
  two_round = FALSE, save_binary = FALSE, sigmoid = 1,
  is_unbalance = FALSE, max_position = 20, label_gain = c(0, 1, 3, 7, 15,
  31, 63), metric = "l2", metric_freq = 1, is_training_metric = FALSE,
  ndcg_at = c(1, 2, 3, 4, 5), num_machines = 1, local_listen_port = 12400,
  time_out = 120, machine_list_file = "",
  lgbm_path = "path/to/LightGBM.exe", workingdir = getwd(),
  files_exist = FALSE, train_conf = "lgbm_train.conf",
  train_name = "lgbm_train.csv", val_name = "lgbm_val.csv",
  unicity = FALSE, prediction = TRUE, pred_conf = "lgbm_pred.conf",
  verbose = TRUE, log_name = NA, log_append = FALSE)
}
\arguments{
\item{y_train}{Type: vector. The training labels.}

\item{x_train}{Type: data.table (preferred), data.frame, or matrix. The training features.}

\item{folds}{Type: vector of integers. The fold assigned to each row.}

\item{application}{Type: character. The label application to learn. Must be either \code{'regression'}, \code{'binary'}, or \code{'lambdarank'}. Defaults to \code{'regression'}.}

\item{validation}{Type: boolean. Whether LightGBM performs validation during the training, by outputting metrics for the validation data. Defaults to \code{TRUE}. Multi-validation data is not supported yet.}

\item{num_iterations}{Type: integer. The number of boosting iterations LightGBM will perform. Defaults to \code{10}.}

\item{early_stopping_rounds}{Type: integer. The number of boosting iterations whose validation metric is lower than the best is required for LightGBM to automatically stop. Defaults to \code{NA}.}

\item{learning_rate}{Type: numeric. The shrinkage rate applied to each iteration. Lower values lowers overfitting speed, while higher values increases overfitting speed. Defaults to \code{0.1}.}

\item{num_leaves}{Type: integer. The number of leaves in one tree. Roughly, a recommended value is \code{n^2 - 1}, \code{n} being the theoretical depth if each tree were identical. Lower values lowers tree complexity, while higher values increases tree complexity. Defaults to \code{127}.}

\item{tree_learner}{Type: character. The type of learner use, between \code{'serial'} (single machine tree learner), \code{'feature'} (feature parallel tree learner), \code{'data'} (data parallel tree learner). Defaults to \code{'serial'}. Other learners are not supported yet. (?)}

\item{num_threads}{Type: integer. The number of threads to run for LightGBM. It is recommended to not set it higher than the amount of physical cores in your computer. Defaults to \code{2}. In virtualized environments, it can be better to set it to the maximum amount of threads allocated to the virtual machine (especially VirtualBox).}

\item{min_data_in_leaf}{Type: integer. Minimum number of data in one leaf. Higher values potentially decrease overfitting. Defaults to \code{100}.}

\item{min_sum_hessian_in_leaf}{Type: numeric. Minimum sum of hessians in one leaf to allow a split. Higher values potentially decrease overfitting. Defaults to \code{10.0}.}

\item{feature_fraction}{Type: numeric (0, 1). Column subsampling percentage. For instance, 0.5 means selecting 50\% of features randomly for each iteration. Lower values potentially decrease overfitting, while training faster. Defaults to \code{1.0}.}

\item{feature_fraction_seed}{Type: integer. Random starting seed for the column subsampling (\code{feature_fraction}). Defaults to \code{2}.}

\item{bagging_fraction}{Type: numeric (0, 1). Row subsampling percentage. For instance, 0.5 means selecting 50\% of rows randomly for each iteration. Lower values potentially decrease overfitting, while training faster. Defaults to \code{1.0}. Unused when \code{bagging_freq} is \code{0}.}

\item{bagging_freq}{Type: integer. The frequency of row subsampling (\code{bagging_fraction}). Lower values potentially decrease overfitting, while training faster. Defaults to \code{0}.}

\item{bagging_seed}{Type: integer. Random starting seed for the row subsampling (\code{bagging_fraction}). Defaults to \code{3}.}

\item{max_bin}{Type: integer. The maximum number of bins created per feature. Lower values potentially decrease overfitting. Defaults to \code{255}.}

\item{data_random_seed}{Type: integer. Random starting seed for the parallel learner. Defaults to \code{1}.}

\item{data_has_label}{Type: boolean. Whether the data has labels or not. Do not modify this. Defaults to \code{TRUE}.}

\item{output_model}{Type: character. The file name of output model. Defaults to \code{'LightGBM_model.txt'}.}

\item{input_model}{Type: characer. The file name of input model. If defined, LightGBM will resume training from that file. Defaults to \code{NA}. Unused yet.}

\item{output_result}{Type: character. The file name of the prediction results for the model. Defaults to \code{'LightGBM_predict_result.txt'}. Unused yet.}

\item{is_sigmoid}{Type: boolean. Whether to use a sigmoid transformation of raw predictions. Defaults to \code{TRUE}.}

\item{init_score}{Type: string. The file name of initial scores to start training LightGBM. Defaults to \code{''}. Automatic creation of the initial scores is not implemented yet.}

\item{is_pre_partition}{Type: boolean. Whether data is pre-partitioned for parallel learning. Defaults to \code{FALSE}. Unused.}

\item{is_sparse}{Type: boolean. Whether sparse optimization is enabled. Defaults to \code{TRUE}.}

\item{two_round}{Type: boolean. LightGBM maps data file to memory and load features from memory to maximize speed. If the data is too large to fit in memory, use TRUE. Defaults to \code{FALSE}.}

\item{save_binary}{Type: boolean. Whether data should be saved as binary files for faster load. Defaults to \code{FALSE}.,}

\item{sigmoid}{Type: numeric. "The sigmoid parameter". Defaults to \code{1.0}.}

\item{is_unbalance}{Type: boolean. For binary classification, setting this to TRUE might be useful when the training data is unbalanced. Defaults to \code{FALSE}.}

\item{max_position}{Type: integer. For lambdarank, optimize NDCG for that specific value. Defaults to \code{20}.}

\item{label_gain}{Type: vector of integers. For lambdarank, relevant gain for labels. Defaults to \code{c(0, 1, 3, 7, 15, 31, 63)}.}

\item{metric}{Type: character, or vector of characters. The metric to optimize. There are 6 available: \code{'l1'} (absolute loss), \code{'l2'} (squared loss), \code{'ndcg'} (NDCG), \code{'auc'} (AUC), \code{'binary_logloss'} (logarithmic loss), and \code{'binary_error'} (accuracy). Defaults to \code{'l2'}. Use a vector of characters to pass multiple metrics.}

\item{metric_freq}{Type: integer. The frequency to report the metric(s). Defaults to \code{1}.}

\item{is_training_metric}{Type: boolean. Whether to report the training metric in addition to the validation metric. Defaults to \code{FALSE}.}

\item{ndcg_at}{Type: vector of integers. Evaluate NDCG metric at these values. Defaults to \code{c(1, 2, 3, 4, 5)}.}

\item{num_machines}{Type: integer. When using parallel learning, the number of machines to use. Defaults to \code{1}.}

\item{local_listen_port}{Type: integer. The TCP listening port for the local machines. Allow this port in the firewall before training. \code{12400}.}

\item{time_out}{Type: integer. The socket time-out in minutes. Defaults to \code{120}.}

\item{machine_list_file}{Type: character. The file that contains the machine list for parallel learning. A line in that file much correspond to one IP and one port for one machine, separated by space instead of a colon (\code{:}). Defaults to \code{''}.}

\item{lgbm_path}{Type: character. Where is stored LightGBM? Include only the folder to it. Defaults to \code{'path/to/LightGBM.exe'}.}

\item{workingdir}{Type: character. The working directory used for LightGBM. Defaults to \code{getwd()}.}

\item{files_exist}{Type: boolean. Whether the files are already existing. It does not export the files anymore if the training and validation files were already exported previously. Defaults to \code{FALSE}.}

\item{train_conf}{Type: character. The name of the train_conf file for the model. Defaults to \code{'lgbm_train.conf'}}

\item{train_name}{Type: character. The name of the training data file for the model. Defaults to \code{'lgbm_train.csv'}}

\item{val_name}{Type: character. The name of the testing data file for the model. Defaults to \code{'lgbm_val.csv'}}

\item{unicity}{Type: boolean. Whether to overwrite each train/validation file. If not, adds a tag to each file. Defaults to \code{TRUE}.}

\item{prediction}{Type: boolean. Whether cross-validated predictions should be returned. Defaults to \code{TRUE}.}

\item{pred_conf}{Type: character. The name of the pred_conf file for the model. Defaults to \code{'lgbm_pred.conf'}}

\item{verbose}{Type: boolean. Whether to print a lot of debug messages or not. Using a defined \code{log_name} and \code{verbose = TRUE} is equivalent to tee (output log to stdout and to a file). 0 is FALSE and 1 is TRUE. 2 can be used if you wish to not separate logs per fold (i.e. all log in one file + print in console), and -1 for not printing in console (keep only log). Defaults to \code{TRUE}. Useless as \code{FALSE} when log_name is not set. Might not work when your lgbm_path has a space. When FALSE, the default printing is diverted to \code{"diverted_verbose.txt"}, but the model log is output to \code{log_name}.}

\item{log_name}{Type: character. The logging (sink) file to output (like 'log.txt'). Defaults to \code{NA}.}

\item{log_append}{Type: boolean. Whether logging should be appended to the log_name or not (not delete or delete old). Defaults to \code{TRUE}.}
}
\value{
If \code{prediction == TRUE}, returns the cross-validated predictions. Otherwise, returns the working directory for the trained models.
}
\description{
This function allows you to cross-validate a LightGBM model.
It is recommended to have your x_train and x_val sets as data.table, and to use the development data.table version.
To install data.table development version, please run in your R console: \code{install.packages("data.table", type = "source", repos = "http://Rdatatable.github.io/data.table")}.
The speed increase to create the train and test files can exceed 100x over write.table in certain cases.
}
\details{
Folder/File specifics:
* \code{lgbm_path} is the path to LightGBM executable, and includes the executable name and tag.
* \code{workingdir} is the working directory for the temporary files for LightGBM. Files will be under \code{'workingdir'}.
* \code{train_conf}, \code{train_name}, and \code{val_name} defines respectively the configuration file name, the train file name, and the validation file name. They are created under this name when \code{files_exist} is set to \code{TRUE}.
* \code{unicity} defines whether to create separate files (if \code{TRUE}) or to save space by writing over the same file (if \code{FALSE}). Predicting does not work with \code{FALSE}.
}
\examples{
#None yet.

}

