% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LossFunctions.R
\name{loss_LKL_xgb}
\alias{loss_LKL_xgb}
\title{Laurae's Kullback-Leibler Error (xgboost function)}
\usage{
loss_LKL_xgb(preds, dtrain)
}
\arguments{
\item{preds}{The \code{predictions}.}

\item{dtrain}{The xgboost model.}
}
\value{
The gradient and the hessian of the Laurae's Kullback-Leibler Error per value in a list.
}
\description{
This function computes for xgboost's \code{obj} function the Laurae's Kullback-Leibler Error loss gradient and hessian per value provided \code{preds} and \code{dtrain}.
}
\details{
This loss function is strictly positive, therefore defined in \code{\]0, +Inf\[}. It penalizes lower values more heavily, and as such is a good fit for typical problems requiring fine tuning when undercommitting on the predictions. Compared to Laurae's Poisson loss function, Laurae's Kullback-Leibler loss has much higher loss. Negative and null values are set to \code{1e-15}. This loss function is experimental.

Loss Formula : \eqn{(y_true - y_pred) * log(y_true / y_pred)}

Gradient Formula : \eqn{-((y_true - y_pred)/y_pred + log(y_true) - log(y_pred))}

Hessian Formula : \eqn{((y_true - y_pred)/y_pred + 2)/y_pred}
}

