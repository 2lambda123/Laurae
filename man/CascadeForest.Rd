% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CascadeForest.R
\name{CascadeForest}
\alias{CascadeForest}
\title{Cascade Forest implementation in R}
\usage{
CascadeForest(training_data, validation_data, training_labels,
  validation_labels, folds, boosting = FALSE, nthread = 1, cascade_lr = 1,
  training_start = NULL, validation_start = NULL, cascade_forests = rep(4,
  5), cascade_trees = 500, cascade_rf = 2, cascade_seeds = 0,
  objective = "reg:linear", eval_metric = "rmse", multi_class = FALSE,
  early_stopping = 2, maximize = FALSE, verbose = TRUE,
  low_memory = FALSE)
}
\arguments{
\item{training_data}{Type: data.table. The training data. Columns are added during training if \code{low_memory == TRUE}, so you may want to clean it up if you use \code{low_memory == TRUE} and interrupt training.}

\item{validation_data}{Type: data.table. The validation data to check for metric performance. Set to \code{NULL} if you want to use out of fold validation data instead of a custom validation data set. Columns are added during training if \code{low_memory == TRUE}, so you may want to clean it up if you use \code{low_memory == TRUE} and interrupt training.}

\item{training_labels}{Type: numeric vector. The training labels.}

\item{validation_labels}{Type: numeric vector. The validation labels.}

\item{folds}{Type: list. The folds as list for cross-validation.}

\item{boosting}{Type: logical. Whether to perform boosting or not for training. It may converge faster, but may overfit faster and therefore needs control via \code{cascade_lr}. Defaults to \code{FALSE}.}

\item{nthread}{Type: numeric. The number of threads using for multithreading. 1 means singlethread (uses only one core). Higher may mean faster training if the memory overhead is not too large. Defaults to \code{1}.}

\item{cascade_lr}{Type: numeric vector or numeric. The shrinkage affected to each tree per layer to avoid overfitting, for each layer. You may specify a vector to change the learning rate per layer, such as \code{c(0.4, 0.3, 0.2, 0.1, 0.05)} so you can perform boosting afterwards. Defaults to \code{1}.}

\item{training_start}{Type: numeric vector. The initial training prediction labels. Set to \code{NULL} if you do not know what you are doing. Defaults to \code{NULL}.}

\item{validation_start}{Type: numeric vector. The initial validation prediction labels. Set to \code{NULL} if you do not know what you are doing. Defaults to \code{NULL}.}

\item{cascade_forests}{Type: numeric vector (mandatory). The number of forest models per layer in the architecture to create for the Cascade Forest. Defaults to \code{rep(4, 5)}.}

\item{cascade_trees}{Type: numeric vector or numeric. The number of trees per forest model per layer in the architecture to create for the Cascade Forest. You may specify a vector to change the learning rate per layer, such as \code{500} so you can perform boosting afterwards. Defaults to \code{1000}.}

\item{cascade_rf}{Type: numeric vector or numeric. The number of Random Forest model per layer in the architecture to create for the Cascade Forest. You may specify a vector to change the learning rate per layer, such as \code{c(1, 1, 2, 3, 5)} so you can perform boosting afterwards. Defaults to \code{2}.}

\item{cascade_seeds}{Type: numeric vector or numeric. Random seed for reproducibility per layer. Defaults to \code{0}.}

\item{objective}{Type: character or function. The function which leads \code{boosting} loss. See \code{xgboost::xgb.train}. Defaults to \code{"reg:linear"}.}

\item{eval_metric}{Type: character or function. The function which evaluates \code{boosting} loss. See \code{xgboost::xgb.train}. Defaults to \code{"rmse"}.}

\item{multi_class}{Type: numeric. Defines the number of classes internally for whether you are doing multi class classification or not to use specific routines for multiclass problems when using \code{return_list == FALSE}. Defaults to \code{2}, which is for regression and binary classification.}

\item{early_stopping}{Type: numeric. Defines how many architecture layers without improvement to require before stopping early (therefore, you must remove 1 to that value - for instance, a stopping of 2 means it will stop after 3 failures to improve). 0 means instantly stop at the first failure for improvement. -1 means no stopping. Requires \code{validation_data} to be able to stop early. Defaults to \code{2}.}

\item{maximize}{Type: logical. Whether to maximize or not the loss evaluation metric. Defaults to \code{FALSE}.}

\item{verbose}{Type: logical. Whether to print training evaluation. Defaults to \code{TRUE}.}

\item{low_memory}{Type: logical. Whether to perform the data.table transformations in place to lower memory usage. Defaults to \code{FALSE}.}
}
\value{
A data.table based on \code{target}.
}
\description{
This function attempts to replicate Cascade Forest using xgboost. It performs Complete-Random Tree Forest in a Neural Network directed acrylic graph like in Neural Networks, but only for simple graphs (e.g use previous layer output data for next layer training each time). You can specify your learning objective using \code{objective} and the metric to check for using \code{eval_metric}. You can plug custom objectives instead of the objectives provided by \code{xgboost}.
}
\details{
For implementation details of Cascade Forest / Complete-Random Tree Forest / Multi-Grained Scanning / Deep Forest, check this: \url{https://github.com/Microsoft/LightGBM/issues/331#issuecomment-283942390} by Laurae.

\itemize{
\item{Complete-Random Tree Forest = ensemble of Random Trees (number of forests parameter)}
\item{Random Tree split = take a random feature each time, grow indefinitely until there are only no more than 10 instances of the same class (or there is only one class at the end) (minimum number of instances parameter)}
\item{Random Tree Forest = take only sqrt(number of features) per tree (column sampling by tree parameter)}
\item{(NOT IMPLEMENTED AS IT IS XGBOOST USED) Split value used = Gini (but I would say using Information Criterion could be better for random forests) (potentially loss function parameter)}
\item{Number of features generated by a level = number of complete-random tree forest * number of classes, in case of regression this would be number of complete-random tree forest}
\item{Each Complete-Random Tree Forest is generated using cross-validation (stacking like in Kaggle competitions) and using average when you predict (parameter - input folds)}
\item{Each time a Cascade Forest is finished, compare to validation set. If the new Cascade Forest is worse than the previous one, terminate training immediately (bonus: early_stopping_rounds would be able to determine how many bad Cascade Forest needed to stop, and maybe prediction should give us the ability to choose how many forests to use?).}
}
}
\examples{
\dontrun{
# Load libraries
library(data.table)
library(Matrix)
library(xgboost)

# Create data
data(agaricus.train, package = "lightgbm")
data(agaricus.test, package = "lightgbm")
agaricus_data_train <- data.table(as.matrix(agaricus.train$data))
agaricus_data_test <- data.table(as.matrix(agaricus.test$data))
agaricus_label_train <- agaricus.train$label
agaricus_label_test <- agaricus.test$label
folds <- Laurae::kfold(agaricus_label_train, 5)

# Train a model (binary classification)
model <- CascadeForest(training_data = agaricus_data_train, # Training data
                       validation_data = agaricus_data_test, # Validation data
                       training_labels = agaricus_label_train, # Training labels
                       validation_labels = agaricus_label_test, # Validation labels
                       folds = folds, # Folds for cross-validation
                       boosting = FALSE, # Do not touch this unless you are expert
                       nthread = 1, # Change this to use more threads
                       cascade_lr = 1, # Do not touch this unless you are expert
                       training_start = NULL, # Do not touch this unless you are expert
                       validation_start = NULL, # Do not touch this unless you are expert
                       cascade_forests = rep(4, 5), # Number of forest models
                       cascade_trees = 1000, # Number of trees per forest
                       cascade_rf = 2, # Number of Random Forest in models
                       cascade_seeds = 0, # Seed per layer
                       objective = "binary:logistic",
                       eval_metric = "logloss",
                       multi_class = 2, # Modify this for multiclass problems
                       early_stopping = 2, # stop after 2 bad combos of forests
                       maximize = FALSE, # not a maximization task
                       verbose = TRUE, # print information during training
                       low_memory = FALSE)

# Attempt to perform fake multiclass problem
agaricus_label_train[1:100] <- 2

# Train a model (multiclass classification)
model <- CascadeForest(training_data = agaricus_data_train, # Training data
                       validation_data = agaricus_data_test, # Validation data
                       training_labels = agaricus_label_train, # Training labels
                       validation_labels = agaricus_label_test, # Validation labels
                       folds = folds, # Folds for cross-validation
                       boosting = FALSE, # Do not touch this unless you are expert
                       nthread = 1, # Change this to use more threads
                       cascade_lr = 1, # Do not touch this unless you are expert
                       training_start = NULL, # Do not touch this unless you are expert
                       validation_start = NULL, # Do not touch this unless you are expert
                       cascade_forests = rep(4, 5), # Number of forest models
                       cascade_trees = 1000, # Number of trees per forest
                       cascade_rf = 2, # Number of Random Forest in models
                       cascade_seeds = 0, # Seed per layer
                       objective = "multi:softprob",
                       eval_metric = "mlogloss",
                       multi_class = 3, # Modify this for multiclass problems
                       early_stopping = 2, # stop after 2 bad combos of forests
                       maximize = FALSE, # not a maximization task
                       verbose = TRUE, # print information during training
                       low_memory = FALSE)
}

}

