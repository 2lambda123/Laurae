% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LossFunctions.R
\name{loss_LKL_grad}
\alias{loss_LKL_grad}
\title{Laurae's Kullback-Leibler Error (gradient function)}
\usage{
loss_LKL_grad(y_pred, y_true)
}
\arguments{
\item{y_pred}{The \code{predictions}.}

\item{y_true}{The \code{labels}.}
}
\value{
The gradient of the Laurae's Kullback-Leibler Error per value.
}
\description{
This function computes the Laurae's Kullback-Leibler loss gradient per value provided \code{preds} and \code{labels} values.
}
\details{
This loss function is strictly positive, therefore defined in \code{\]0, +Inf\[}. It penalizes lower values more heavily, and as such is a good fit for typical problems requiring fine tuning when undercommitting on the predictions. Compared to Laurae's Poisson loss function, Laurae's Kullback-Leibler loss has much higher loss. This loss function is experimental.

Loss Formula : \eqn{(y_true - y_pred) * log(y_true / y_pred)}

Gradient Formula : \eqn{-((y_true - y_pred)/y_pred + log(y_true) - log(y_pred))}

Hessian Formula : \eqn{((y_true - y_pred)/y_pred + 2)/y_pred}
}

