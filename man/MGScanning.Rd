% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MGScanning.R
\name{MGScanning}
\alias{MGScanning}
\title{Multi-Grained Scanning implementation in R}
\usage{
MGScanning(data, labels, folds, dimensions = 1, depth = 10, stride = 1,
  nthread = 1, lr = 1, training_start = NULL, validation_start = NULL,
  n_forest = 2, n_trees = 30, random_forest = 1, seed = 0,
  objective = "reg:linear", eval_metric = Laurae::df_rmse,
  multi_class = 2, verbose = TRUE)
}
\arguments{
\item{data}{Type: data.table (\code{dimensions == 1}) or list of matrices (\code{dimensions == 2}). The training data.}

\item{labels}{Type: numeric vector. The training labels.}

\item{folds}{Type: list. The folds as list for cross-validation.}

\item{dimensions}{Type: numeric. The dimensions of the data. Only supported is \code{1} for matrix format, and \code{2} for list of matrices. Defaults to \code{1}.}

\item{depth}{Type: numeric. The size of the sliding window applied. Use a vector of size 2 when using two dimensions (row, col). Do not make it larger than \code{ncol(data)} when \code{dimensions == 1}, or when \code{dimensions == 2} the \code{depth} must be smaller than the height and width of each matrix. Defaults to \code{2}.}

\item{stride}{Type: numeric. The stride (sliding steps) applied to each sliding window. Use a vector of size 2 when using two dimensions (row, col). Defaults to \code{1}.}

\item{nthread}{Type: numeric. The number of threads using for multithreading. 1 means singlethread (uses only one core). Higher may mean faster training if the memory overhead is not too large. Defaults to \code{1}.}

\item{lr}{Type: numeric. The shrinkage affected to each tree to avoid overfitting. Defaults to \code{1}, which means no adjustment.}

\item{training_start}{Type: numeric vector. The initial training prediction labels. Set to \code{NULL} if you do not know what you are doing. Defaults to \code{NULL}.}

\item{validation_start}{Type: numeric vector. The initial validation prediction labels. Set to \code{NULL} if you do not know what you are doing. Defaults to \code{NULL}.}

\item{n_forest}{Type: numeric. The number of forest models to create for the Complete-Random Tree Forest. Defaults to \code{5}.}

\item{n_trees}{Type: numeric. The number of trees per forest model to create for the Complete-Random Tree Forest. Defaults to \code{1000}.}

\item{random_forest}{Type: numeric. The number of Random Forest in the forest. Defaults to \code{0}.}

\item{seed}{Type: numeric. Random seed for reproducibility. Defaults to \code{0}.}

\item{objective}{Type: character or function. The function which leads \code{boosting} loss. See \code{xgboost::xgb.train}. Defaults to \code{"reg:linear"}.}

\item{eval_metric}{Type: function. The function which evaluates \code{boosting} loss. Must take two arguments in the following order: \code{preds, labels} (they may be named in another way) and returns a metric. Defaults to \code{Laurae::df_rmse}.}

\item{multi_class}{Type: logical. Defines internally whether you are doing multi class classification or not to use specific routines for multiclass problems when using \code{return_list == FALSE}. Defaults to \code{FALSE}.}

\item{verbose}{Type: character. Whether to print for training evaluation. Use \code{""} for no printing (double quotes without space between quotes). Defaults to \code{" "} (double quotes with space between quotes.}
}
\value{
A data.table based on \code{target}.
}
\description{
This function attempts to replicate Multi-Grained Scanning using xgboost. It performs Random Forest \code{n_forest} times using \code{n_trees} trees on your data using a sliding window to create features. You can specify your learning objective using \code{objective} and the metric to check for using \code{eval_metric}. You can plug custom objectives instead of the objectives provided by \code{xgboost}. As with any uncalibrated machine learning methods, this method suffers uncalibrated outputs. Therefore, the usage of scale-dependent metrics is discouraged (please use scale-invariant metrics, such as Accuracy, AUC, R-squared, Spearman correlation...).
}
\details{
For implementation details of Cascade Forest / Complete-Random Tree Forest / Multi-Grained Scanning / Deep Forest, check this: \url{https://github.com/Microsoft/LightGBM/issues/331#issuecomment-283942390} by Laurae.

\itemize{
\item{Complete-Random Tree Forest = ensemble of Random Trees (number of forests parameter)}
\item{Random Tree split = take a random feature each time, grow indefinitely until there are only no more than 10 instances of the same class (or there is only one class at the end) (minimum number of instances parameter)}
\item{Random Tree Forest = take only sqrt(number of features) per tree (column sampling by tree parameter)}
\item{(NOT IMPLEMENTED AS IT IS XGBOOST USED) Split value used = Gini (but I would say using Information Criterion could be better for random forests) (potentially loss function parameter)}
\item{Number of features generated by a level = number of complete-random tree forest * number of classes, in case of regression this would be number of complete-random tree forest}
\item{Each Complete-Random Tree Forest is generated using cross-validation (stacking like in Kaggle competitions) and using average when you predict (parameter - input folds)}
\item{Each time a Cascade Forest is finished, compare to validation set. If the new Cascade Forest is worse than the previous one, terminate training immediately (bonus: early_stopping_rounds would be able to determine how many bad Cascade Forest needed to stop, and maybe prediction should give us the ability to choose how many forests to use?).}
}
}
\examples{
\dontrun{
# Load libraries
library(data.table)
library(Matrix)
library(xgboost)

# Create data
data(agaricus.train, package = "lightgbm")
data(agaricus.test, package = "lightgbm")
agaricus_data_train <- data.table(as.matrix(agaricus.train$data))
agaricus_data_test <- data.table(as.matrix(agaricus.test$data))
agaricus_label_train <- agaricus.train$label
agaricus_label_test <- agaricus.test$label
folds <- Laurae::kfold(agaricus_label_train, 5)

# Train a model (binary classification)
model <- MGScanning(data = agaricus_data_train, # Training data
                    labels = agaricus_label_train, # Training labels
                    folds = folds, # Folds for cross-validation
                    dimensions = 1, # Change this for 2 dimensions if needed
                    depth = 10, # Change this to change the sliding window size
                    stride = 1, # Change this to change the sliding window speed
                    nthread = 1, # Change this to use more threads
                    lr = 1, # Do not touch this unless you are expert
                    training_start = NULL, # Do not touch this unless you are expert
                    validation_start = NULL, # Do not touch this unless you are expert
                    n_forest = 2, # Number of forest models
                    n_trees = 30, # Number of trees per forest
                    random_forest = 1, # We want only 2 random forest
                    seed = 0,
                    objective = "binary:logistic",
                    eval_metric = Laurae::df_logloss,
                    multi_class = 2, # Modify this for multiclass problems)
                    verbose = TRUE)

# Create predictions
data_predictions <- model$preds

# Example on fake pictures (matrices) and multiclass problem

# Generate fake images
new_data <- list(matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20),
                 matrix(rnorm(n = 400), ncol = 20, nrow = 20))

# Generate fake labels
new_labels <- c(2, 1, 0, 2, 1, 0, 2, 1, 0, 0)

# Train a model (multiclass problem)
model <- MGScanning(data = new_data, # Training data
                    labels = new_labels, # Training labels
                    folds = list(1:3, 3:6, 7:10), # Folds for cross-validation
                    dimensions = 2,
                    depth = 10,
                    stride = 1,
                    nthread = 1, # Change this to use more threads
                    lr = 1, # Do not touch this unless you are expert
                    training_start = NULL, # Do not touch this unless you are expert
                    validation_start = NULL, # Do not touch this unless you are expert
                    n_forest = 2, # Number of forest models
                    n_trees = 10, # Number of trees per forest
                    random_forest = 1, # We want only 2 random forest
                    seed = 0,
                    objective = "multi:softprob",
                    eval_metric = Laurae::df_logloss,
                    multi_class = 3, # Modify this for multiclass problems)
                    verbose = TRUE)

# Matrix output is 10x600
dim(model$preds)
}

}

