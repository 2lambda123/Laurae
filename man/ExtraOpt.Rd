% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ExtraOpt.R
\name{ExtraOpt}
\alias{ExtraOpt}
\title{Cross-Entropy -based Hybrid Optimization}
\usage{
ExtraOpt(f_train = .ExtraOpt_trainer, ..., f_est = .ExtraOpt_estimate,
  f_prob = .ExtraOpt_prob, preInit = NULL, Ninit = 50L, Nmax = 200,
  Nimprove = 10, elites = 0.9, max_elites = 150, tested_elites = 5,
  elites_converge = 10, maximize = TRUE, best = NULL, cMean = NULL,
  cSD = NULL, cOrdinal = NULL, cMin = NULL, cMax = NULL, cThr = 0.001,
  dProb = NULL, dThr = 0.999, priorsC = NULL, priorsD = NULL,
  errorCode = -9999, autoExpVar = FALSE, autoExpFile = NULL,
  verbose = 1, plot = NULL, debug = FALSE)
}
\arguments{
\item{f_train}{Type: function. The training function which returns at the end the loss. All arguments provided to \code{ExtraOpt} in \code{...} are provided to \code{f_train}. Defaults to \code{.ExtraOpt_trainer}, which is a sample xgboost trainer.}

\item{...}{Type: any. Arguments to pass to \code{f_train}.}

\item{f_est}{Type: function. The estimator supervised machine learning function for the variables to optimize. It must return a list with \code{Model} as the model to use for \code{f_prob}, and the \code{Error} as the loss of the estimator model. Defaults to \code{.ExtraOpt_estimate}, which is a sample xgboost variable estimator.}

\item{f_prob}{Type: function. The predictor function for the supervised machine learning function. It takes the model from \code{f_est} and a prior vector as inputs, and returns the predicted loss from \code{f_est}. Defaults to \code{.ExtraOpt_prob}, which is a sample xgboost estimator prediction.}

\item{preInit}{Type: boolean. Whether a prior list is already computed to be used instead of the initiailzation. Set Ninit accordingly if you use a pre-initialized priors matrix. Defaults to \code{NULL}.}

\item{Ninit}{Type: integer. The initialization amount. It is best to use at least 2 times the number of initialization vs the number of variables. For instance, 50 features should require \code{Ninit = 100}, even if it does not guarantee a best result.}

\item{Nmax}{Type: integer. The maximum number of iterations alloted to optimize the variables provided against the loss. Once this amount of iterations is reached (excluding error code iterations), the function stops. Defaults to \code{200}.}

\item{Nimprove}{Type: integer. The maximum number of iterations alloted to optimize without improvements. Defaults to \code{10}.}

\item{elites}{Type: numeric. The percentage of iteration samples retained in the parameter estimator. The larger the \code{elites}, the lower the ability to get stuck at a local optima. However, a very low elite amount would get quickly stuck at a local optima and potentially overfit. After the initialization, a minimum of 5 sampled elites is mandatory. For instance, if \code{Ninit = 100}, then \code{elites >= 0.05}. It should do be higher than \code{1}. If the sampling results in a decimal-valued numeric, it will take the largest value. If the sampling results in a lower than \code{5} numeric, it will shrink back to \code{5}. Defaults to \code{0.90}.}

\item{max_elites}{Type: numeric. The maximum allowed number of elite samples. Setting this value low increases the convergence speed, at the expense of exploration. It is not recommended to increase it over \code{5000} as it will slow down severely the next prior optimization. When elites have the same loss, the elite which was computed the earliest takes precedence over all others identical-loss elites (even if their parameters are different). Defaults to \code{150}.}

\item{tested_elites}{Type: numeric. The number of elites tested at the same time when trying to find new values. A high value increases the space exploration at the expense of convergence speed. Minimum of \code{1} for small steps but fast convergence speed, supposing the initialization with good enough. Defaults to \code{5}.}

\item{elites_converge}{Type: numeric. The number of elites to use to assess convergence via \code{cThr} and \code{dThr}. The larger the \code{elites_converge}, the tighter the convergence requirements. It cannot be higher than the number of \code{tested_elites}. Defaults to \code{10}.}

\item{maximize}{Type: boolean. Whether to maximize or not to maximize (minimize). Defaults to \code{TRUE}.}

\item{best}{Type: numeric. The best value you can get from the loss you will accept to interrupt early the optimizer. Defaults to \code{NULL}.}

\item{cMean}{Type: numeric vector. The mean of continuous variables to feed to \code{f_train}.}

\item{cSD}{Type: numeric vector. The standard deviation of continuous variables to feed to \code{f_train}.}

\item{cOrdinal}{Type: boolean vector. Whether each continuous variable is ordinal or not.}

\item{cMin}{Type: numeric vector. The minimum of each continuous variable.}

\item{cMax}{Type: numeric vector. The maximum of each continuous variable.}

\item{cThr}{Type: numeric. The value at which if the maximum standard deviation of continuous variables of the elites is higher than \code{cThr}, the continuous variables are supposed having converged. Once converged, the algorithm will have only one try to generate a higher threshold while optimizing. If it fails, convergence interrupts the optimization. Applies also to the cross-entropy internal optimization. Defaults to \code{0.001}, which means the continuous variables will be supposed converged once there is no more maximum standard deviation of 0.001.}

\item{dProb}{Type: list of numeric vectors. A list containing for each discrete variable, a vector with the probability of the \code{i-1}-th element to appear.}

\item{dThr}{Type: numeric. The value at which if the probability of the worst occurring discrete value in discrete variables of the elites is higher or equal to \code{dThr}, the discrete variables are supposed having converged. Once converged, the algorithm will have only one try to generate a higher threshold while optimizing. If it fails, convergence interrupts the optimization. Applies also to the cross-entropy internal optimization, but as \code{1 - dThr}. Defaults to \code{1}, which means the discrete variables will be supposed converged once all discrete variables have the same probability of 1.}

\item{priorsC}{Type: matrix. The matrix of continuous priors. Even when filled, \code{cMean} and \code{cSD} are mandatory to be filled.}

\item{priorsD}{Type: matrix. The matrix of discrete priors. Even when filled, \code{dProb} is mandatory to be filled.}

\item{errorCode}{Type: vector of 2 numerics. When f_train is ill-conditioned or has an "error", you can use an error code to replace it by a dummy value which will be parsed afterwards for removal. You must adapt it to your own error code. For instance, the error code returned by \code{f_train} should be the \code{errorCode} value when no features are selected for training a supervised model. The error codes are removed from the priors. Defaults to \code{-9999}.}

\item{autoExpVar}{Type: boolean. Whether the local priors must be exported to the global environment. This is extremely useful for debugging, but also to catch the \code{priorsC} and \code{priorsD} matrices when \code{ExtraOpt}, \code{f_train}, \code{f_est}, or \code{f_prob} is error-ing without a possible recovery. You would then be able to feed the priors and re-run without having to run again the algorithm from scratch. Defaults to \code{FALSE}. The saved variable in the global environment is called "temporary_Laurae".}

\item{autoExpFile}{Type: character. Whether the local priors must be exported to as RDS files. This is extremely useful for debugging, but also to catch the \code{priorsC} and \code{priorsD} matrices when \code{ExtraOpt}, \code{f_train}, \code{f_est}, or \code{f_prob} is error-ing without a possible recovery. You would then be able to feed the priors and re-run without having to run again the algorithm from scratch. Defaults to \code{NULL}.}

\item{verbose}{Type: integer. Should \code{ExtraOpt} become chatty and report a lot? A value of \code{0} defines silent, while \code{1} chats a little bit (and \code{2} chats a lot). \code{3} is so chatty it will flood severely. Defaults to \code{1}.}

\item{plot}{Type: function. Whether to call a function to plot data or not. Your plotting function should take as first argument \code{"priors"}, which as a matrix with as first column the \code{Loss}, followed then by continuous variables, and ends with discrete variables. Continuous variables start with \code{"C"} while discrete variables start with \code{"D"} in the column names. Defaults to \code{NULL}.}

\item{debug}{Type: boolean. Whether an interactive console should be used to run line by line for debugging purposes. Defaults to \code{FALSE}.}
}
\value{
A list with \code{best} for the best value found, \code{variables} for the variable values (split into \code{continuous} list and \code{discrete} list), \code{priors} for the list of iterations and their values, \code{elite_priors} for the laste elites used, \code{new_priors} for the last iterations issued from the elites, \code{iterations} for the number of iterations, and \code{thresh_stats} for the threshold statistics over batches.
}
\description{
This function allows to optimize for any input value: continuous, ordinal, discrete/categorical. Simplex-constrained-type optimization is not yet implemented (mutlivariate constraints which are not univariate constraints are not yet implemented). It tries to keep the discrete distribution, and as such, can be used to reduce dimensionality of supervised machine learning model (feature selection) while optimizing the performance. To get an overview of how to structure your functions to use (you need 3!!), check \code{.ExtraOpt_trainer}, \code{.ExtraOpt_estimate}, and \code{.ExtraOpt_prob}. For plotting, check \code{.ExtraOpt_plot} for an example.
}
\examples{
\dontrun{
# Example of params:
- 50 random initializations
- 200 maximum tries
- 3 continuous variables in [0, 10]
--- with 2 continuous and 1 ordinal
--- with respective means (2, 4, 6)
--- and standard deviation (1, 2, 3)
- and 2 discrete features
- with respective prior probabilities {(0.8, 0.2), (0.7, 0.1, 0.2)}
- and loss error code (illegal priors) of -9999

ExtraOpt(Ninit = 50,
         nthreads = 1,
         eta = 0.1,
         early_stop = 10,
         X_train,
         X_test,
         Y_train,
         Y_test,
         Nmax = 200,
         cMean = c(2, 4, 6),
         cSD = c(1, 2, 3),
         cOrdinal = c(FALSE, FALSE, TRUE),
         cMin = c(0, 0, 0),
         cMax = c(10, 10, 10),
         dProb = list(v1 = c(0.8, 0.2), v2 = c(0.7, 0.1, 0.2)),
         priorsC = NULL,
         priorsD = NULL,
         autoExp = FALSE,
         errorCode = -9999)
}

}

